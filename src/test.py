import itertools
import math
import os
import sys
import time

from collections import defaultdict, Counter
from pyspark import SparkContext


# Spark configuration
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
sc = SparkContext('local[*]', 'task1')


def count_dict(items, support_value):
    item_cnt = {item: 0 for item in set(items)}
    for item in items:
        item_cnt[item] += 1
    count_filtered = {k: v for k, v in item_cnt.items() if v >= support_value}
    return count_filtered


def count_dict_tuple(work_chunk, support_value, work_candidates):
    tuple_cnt = defaultdict(int)
    for basket in work_chunk:
        for candidate in work_candidates:
            if set(candidate).issubset(basket):
                tuple_cnt[candidate] += 1
    count_filtered = {k: v for k, v in tuple_cnt.items() if v >= support_value}
    return count_filtered


def get_frequent_candidates(count_dict_input, support_value, tuple_flag):
    frequent_candidate_list = [candidate for candidate, count in count_dict_input.items() if count >= support_value]
    # Update temp result for printing
    for candidate, count in count_dict_input.items():
        if count >= support_value:
            pass_1_temp.append((candidate if tuple_flag else tuple({candidate}), 1))
    return frequent_candidate_list


def generate_candidates(frequent_item, tuple_size):
    candidates = set()
    previous_candidates = set()
    for item_x in frequent_item:
        for item_y in frequent_item:
            if item_x != item_y and len(set(item_x).intersection(set(item_y))) == tuple_size - 2:
                new_set = tuple(sorted(set(item_x).union(set(item_y))))
                if len(new_set) == tuple_size and new_set not in candidates:
                    if not previous_candidates or all(tuple(sorted(candidate)) in frequent_item for candidate in
                                                      itertools.combinations(new_set, tuple_size - 1)):
                        candidates.add(new_set)
        previous_candidates.update(itertools.combinations(item_x, tuple_size - 1))
    return list(candidates)


def apriori(basket_input):
    task_basket = list(basket_input)
    scaled_support = support * (len(task_basket) / num_transaction)

    # Create list of all items in baskets
    items = [item for basket in task_basket for item in basket]

    # Create count dictionary for items
    item_counts = Counter(items)
    item_counts = {k: v for k, v in item_counts.items() if v >= scaled_support}

    # Find frequent item sets of k = 1
    frequent_items = get_frequent_candidates(item_counts, scaled_support, False)

    # Indicator of k item sets
    # Current step: k = 2
    # Generating pairs of frequent items
    candidates = list(itertools.combinations(sorted(frequent_items), 2))
    frequent_items_cnt = count_dict_tuple(task_basket, scaled_support, candidates)
    frequent_items = get_frequent_candidates(frequent_items_cnt, scaled_support, True)

    # Start generating frequent item sets of size 3 and above
    k = 3
    while len(frequent_items) > 0:
        candidates = generate_candidates(frequent_items, k)
        frequent_items_cnt = count_dict_tuple(task_basket, scaled_support, candidates)
        frequent_items = get_frequent_candidates(frequent_items_cnt, scaled_support, True)
        k += 1
    return [pass_1_temp]


def find_frequent_itemsets(candidate, support_value, transaction_list):
    frequent_item_count = defaultdict(int)
    for transaction in transaction_list:
        if all(item in transaction for item in candidate):
            frequent_item_count[candidate] += 1

    frequent_itemsets = [(tuple(sorted(itemset)), count) for itemset, count in frequent_item_count.items()
                         if count >= support_value]
    return frequent_itemsets


def format_string(input_tuple, size, output):
    if len(input_tuple) > size:
        return output + "\n\n"

    if len(input_tuple) == 1:
        output += f"('{input_tuple[0]}'),"
    else:
        output += f"{input_tuple},"

    return output


if __name__ == "__main__":
    start_time = time.time()
    case = int(sys.argv[1])
    support = int(sys.argv[2])
    input_file_path = sys.argv[3]
    output_file_path = sys.argv[4]

    # For testing locally
    # case = 1
    # support = 4
    # input_file_path = "../resource/asnlib/publicdata/small1.csv"
    # output_file_path = "output_task1.txt"

    basket_rdd = sc.textFile(input_file_path)
    header = basket_rdd.first()

    # Preprocessing data based on case
    if case == 1:
        basket_rdd = basket_rdd.filter(lambda x: x != header)\
            .map(lambda line: (line.split(",")[0], line.split(",")[1]))\
            .groupByKey()\
            .mapValues(list)\
            .map(lambda x: x[1])
    elif case == 2:
        basket_rdd = basket_rdd.filter(lambda x: x != header)\
            .map(lambda line: (line.split(",")[1], line.split(",")[0]))\
            .groupByKey()\
            .mapValues(list)\
            .map(lambda x: x[1])

    # Work on the basket list
    basket_list = basket_rdd.collect()
    num_transaction = len(basket_list)

    pass_1_temp = list()
    pass_1_mapper = basket_rdd.mapPartitions(apriori).flatMap(lambda x: x)
    pass_1_reducer = pass_1_mapper.reduceByKey(lambda x, y: x + y)

    pass_2_mapper = pass_1_reducer.map(lambda x: find_frequent_itemsets(x[0], support, basket_list)).flatMap(lambda x: x)
    print(pass_2_mapper.collect())
    pass_2_reducer = pass_2_mapper.filter(lambda x: x[1] >= support).map(lambda x: x[0]).collect()

    pass_1_final = ""
    pass_1_result = sorted(sorted(pass_1_reducer.map(lambda x: x[0]).collect()),
                           key=lambda element: (len(element), element))
    size = len(pass_1_result[0])
    for x in pass_1_result:
        if len(x) != size:
            pass_1_final = pass_1_final[:-1] + "\n\n"
            size = len(x)
        pass_1_final = format_string(x, size, pass_1_final)

    pass_2_final = ""
    pass_2_result = sorted(sorted(pass_2_reducer),
                           key=lambda element: (len(element), element))
    size = len(pass_2_result[0])
    for x in pass_2_result:
        if len(x) != size:
            pass_2_final = pass_2_final[:-1] + "\n\n"
            size = len(x)
        pass_2_final = format_string(x, size, pass_2_final)

    with open(output_file_path, 'w') as file:
        file.write("Candidates:\n" + pass_1_final[:-1] + "\n\nFrequent Itemsets:\n" + pass_2_final[:-1])

    print("Duration:", time.time() - start_time)